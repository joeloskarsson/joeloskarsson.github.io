<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://joeloskarsson.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joeloskarsson.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-20T17:48:25+00:00</updated><id>https://joeloskarsson.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal webpage of Joel Oskarsson. </subtitle><entry><title type="html">ICML Paper “Scalable Deep Gaussian Markov Random Fields for General Graphs” + Workshop Paper on Temporal GNNs</title><link href="https://joeloskarsson.github.io/blog/2022/icml22/" rel="alternate" type="text/html" title="ICML Paper “Scalable Deep Gaussian Markov Random Fields for General Graphs” + Workshop Paper on Temporal GNNs"/><published>2022-07-04T10:00:00+00:00</published><updated>2022-07-04T10:00:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2022/icml22</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2022/icml22/"><![CDATA[<p>At <a href="https://icml.cc/">ICML</a> this summer I am excited to be presenting one paper at the main conference and one workshop paper. Both works are together with my supervisors Per Sidén and Fredrik Lindsten.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/graph_dgmrf_collage-480.webp 480w,/assets/img/graph_dgmrf_collage-800.webp 800w,/assets/img/graph_dgmrf_collage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/graph_dgmrf_collage.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Our paper <a href="https://arxiv.org/abs/2206.05032"><em>Scalable Deep Gaussian Markov tandom Fields for General Graphs</em></a> proposes a scalable Bayesian model for graph-structured data. We build on the <a href="https://arxiv.org/abs/2002.07467">Deep GMRF</a> framework and extend this to a general graph setting.</p> <h3 id="abstract">Abstract</h3> <p>Machine learning methods on graphs have proven useful in many applications due to their ability to handle generally structured data. The framework of Gaussian Markov Random Fields (GMRFs) provides a principled way to define Gaussian models on graphs by utilizing their sparsity structure. We propose a flexible GMRF model for general graphs built on the multi-layer structure of Deep GMRFs, originally proposed for lattice graphs only. By designing a new type of layer we enable the model to scale to large graphs. The layer is constructed to allow for efficient training using variational inference and existing software frameworks for Graph Neural Networks. For a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. This allows for making predictions with accompanying uncertainty estimates. The usefulness of the proposed model is verified by experiments on a number of synthetic and real world datasets, where it compares favorably to other both Bayesian and deep learning methods.</p> <p><a href="https://arxiv.org/abs/2206.05032">Paper</a></p> <p><a href="https://github.com/joeloskarsson/graph-dgmrf">Code</a></p> <p><br/></p> <h2 id="temporal-graph-neural-networks-with-time-continuous-latent-states">Temporal Graph Neural Networks with Time-Continuous Latent States</h2> <p>Additionally, our paper <em>Temporal Graph Neural Networks with Time-Continuous Latent States</em> has been accepted to the <a href="https://sites.google.com/view/continuous-time-methods-icml/home">ICML Workshop on Continuous Time Methods for Machine Learning</a>. In this work we tackle the problem of irregular observations in temporal graph data. The proposed solution is a temporal GNN model with latent states defined over continuous time.</p> <h3 id="abstract-1">Abstract</h3> <p>We propose a temporal graph neural network model for graph-structured irregular time series. The model is designed to handle both irregular time steps and partial graph observations. This is achieved by introducing a time-continuous latent state in each node of the graph. The latent dynamics are defined using a state-dependent decay-mechanism. Observations in the graph neighborhood are taken into account by integrating graph neural network layers in both the state update and predictive model. Experiments on a traffic forecasting task validate the usefulness of both the graph structure and time-continuous dynamics in this setting.</p> <p><a href="https://drive.google.com/file/d/1vQW0UUXXgOExSkEUmpHSgbKQXf-66yns/view?usp=sharing">Paper</a></p> <p><a href="https://github.com/joeloskarsson/continuous-temporal-gnn">Code</a></p>]]></content><author><name>Joel Oskarsson</name></author><category term="paper"/><category term="graphs"/><category term="icml"/><category term="ml"/><summary type="html"><![CDATA[At ICML this summer I am excited to be presenting one paper at the main conference and one workshop paper. Both works are together with my supervisors Per Sidén and Fredrik Lindsten.]]></summary></entry><entry><title type="html">Master’s Thesis: Probabilistic Regression using Conditional Generative Adversarial Networks</title><link href="https://joeloskarsson.github.io/blog/2020/masters-thesis/" rel="alternate" type="text/html" title="Master’s Thesis: Probabilistic Regression using Conditional Generative Adversarial Networks"/><published>2020-08-29T20:48:00+00:00</published><updated>2020-08-29T20:48:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2020/masters-thesis</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2020/masters-thesis/"><![CDATA[<p>I have finished my master’s thesis: <a href="http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-166637">Probabilistic Regression using Conditional Generative Adversarial Networks</a>.</p> <h3 id="abstract">Abstract</h3> <p>Regression is a central problem in statistics and machine learning with applications everywhere in science and technology. In probabilistic regression the relationship between a set of features and a real-valued target variable is modelled as a conditional probability distribution. There are cases where this distribution is very complex and not properly captured by simple approximations, such as assuming a normal distribution. This thesis investigates how conditional Generative Adversarial Networks (GANs) can be used to properly capture more complex conditional distributions. GANs have seen great success in generating complex high-dimensional data, but less work has been done on their use for regression problems. This thesis presents experiments to better understand how conditional GANs can be used in probabilistic regression. Different versions of GANs are extended to the conditional case and evaluated on synthetic and real datasets. It is shown that conditional GANs can learn to estimate a wide range of different distributions and be competitive with existing probabilistic regression models.</p> <p>Code is available <a href="https://github.com/joeloskarsson/CGAN-regression">here</a>.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="thesis"/><category term="gan"/><category term="ml"/><category term="generative"/><summary type="html"><![CDATA[I have finished my master’s thesis: Probabilistic Regression using Conditional Generative Adversarial Networks.]]></summary></entry><entry><title type="html">Learning Communication in Multi-Agent Reinforcement Learning</title><link href="https://joeloskarsson.github.io/blog/2020/marl-communication/" rel="alternate" type="text/html" title="Learning Communication in Multi-Agent Reinforcement Learning"/><published>2020-03-08T09:13:00+00:00</published><updated>2020-03-08T09:13:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2020/marl-communication</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2020/marl-communication/"><![CDATA[<p>In the context of a course on multi agent systems this fall I have been looking into multi agent reinforcement learning. In particular I have explored the problem of learning communication between agents. This resulted in a short report that gives a concise introduction and overview of the research area.</p> <p>The report is written as a survey paper, covering the motivation for learning communication, some underlying theory of reinforcement learning and an overview of existing approaches to the problem. It also includes a discussion part with some thoughts on practical applications and future research.</p> <p>The report is avaliable <a href="/assets/pdf/learning_communication_report.pdf">here</a> and some slides with descriptive illustrations can be found <a href="/assets/pdf/learning_communication_slides.pdf">here</a>.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="rl"/><category term="communication"/><category term="presentation"/><category term="report"/><category term="ml"/><summary type="html"><![CDATA[In the context of a course on multi agent systems this fall I have been looking into multi agent reinforcement learning. In particular I have explored the problem of learning communication between agents. This resulted in a short report that gives a concise introduction and overview of the research area.]]></summary></entry><entry><title type="html">Gridworld Sandbox for Reinforcement Learning</title><link href="https://joeloskarsson.github.io/blog/2019/rl-sandbox/" rel="alternate" type="text/html" title="Gridworld Sandbox for Reinforcement Learning"/><published>2019-02-24T09:05:00+00:00</published><updated>2019-02-24T09:05:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2019/rl-sandbox</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2019/rl-sandbox/"><![CDATA[<p>I have always found that one of the best ways to truly understand a concept is to implement it in code. So when I got a couple days off in between my exams and the spring semester I decided to revisit one of the most interesting topics I studied this fall, but never really had the time to dive deep into amidst term exams and project deadlines. That is the topic of reinforcement learning, or more specifically the <a href="https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)">model-free</a> approach of <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>.</p> <p>This project is a small application written in Python 3 that simulates a gridworld environment and an agent that can be trained using Q-learning. All code can be found on <a href="https://github.com/joeloskarsson/rl-gridworld">github</a>. The environment and learning hyperparameters can be adjusted in the GUI. Training can be followed in real time as the agent moves around the environment. Once a good estimate of the Q-function has been learned an optimal policy w.r.t. the learned function can be used.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="https://raw.githubusercontent.com/joeloskarsson/rl-gridworld/master/screenshots/screenshot0-480.webp 480w,https://raw.githubusercontent.com/joeloskarsson/rl-gridworld/master/screenshots/screenshot0-800.webp 800w,https://raw.githubusercontent.com/joeloskarsson/rl-gridworld/master/screenshots/screenshot0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://raw.githubusercontent.com/joeloskarsson/rl-gridworld/master/screenshots/screenshot0.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Screenshot of gridworld environment </div> <p>The GUI was written in <a href="https://wiki.python.org/moin/PyQt">PyQt</a>, a library that I haven’t worked with before. I have some bad experiences with python GUI libraries (read Tkinter), but I have to say that working with PyQt was really smooth. Qt is a powerful toolkit and PyQt has shown to be a quite accessible binding. I had a really easy time to get up and running and make the GUI look how I wanted, so PyQt will definitely be my go to Python GUI library for future projects.</p> <p>My continued exploration of the area of reinforcement learning has led me more into deep RL. I found a great resource in Open Ai’s “<a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up in Deep RL</a>” that I look forward to explore.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="rl"/><category term="ml"/><summary type="html"><![CDATA[I have always found that one of the best ways to truly understand a concept is to implement it in code. So when I got a couple days off in between my exams and the spring semester I decided to revisit one of the most interesting topics I studied this fall, but never really had the time to dive deep into amidst term exams and project deadlines. That is the topic of reinforcement learning, or more specifically the model-free approach of Q-learning.]]></summary></entry><entry><title type="html">Machine Learning for Transport Mode Classification</title><link href="https://joeloskarsson.github.io/blog/2019/transport-mode/" rel="alternate" type="text/html" title="Machine Learning for Transport Mode Classification"/><published>2019-02-03T16:28:00+00:00</published><updated>2019-02-03T16:28:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2019/transport-mode</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2019/transport-mode/"><![CDATA[<p>This fall I encountered the quite interesting problem of inferring what mode of transportation a person is using based on sensor data. The context for this was my smart energy course. The course is broadly about using ICT to enable sustainable energy usage and one of our assigned projects was to develop an android app to let people track their daily energy usage from transportation. For this me and my group trained a classifier based on speed measurements from the smartphone GPS.</p> <p>There is plenty of research on this topic and I don’t claim to provide a complete coverage of the area. See this rather as a short introduction to an interesting application of machine learning. For a more academic resource I refer to <a href="https://infoscience.epfl.ch/record/229181/">this great review paper</a> by Nikolić and Bierlaire.</p> <h3 id="the-problem-of-transport-mode-classification">The Problem of Transport Mode Classification</h3> <p>Transport mode classification is the problem of based on some data classifying what mode of transportation a person is currently using. In this setting that data mainly comes from a carried smartphone. How specific the classification should be can be different in different applications, but some categories generally considered are:</p> <ul> <li>Walking</li> <li>Bicycle</li> <li>Car</li> <li>Bus</li> <li>Tram</li> <li>Train</li> </ul> <p>Sometimes groups of these are considered, for example motorized or not.</p> <h3 id="sensors-and-data">Sensors and Data</h3> <p>Typically data from smartphone sensors is used. This makes sense since most people would already carry their smartphone while traveling. Some type of specialized hardware could however also be considered.</p> <p>The main pieces of data used are speed and acceleration, based on position measurements. Speed and acceleration can be estimated from numerical differentiation of position, but one can consider using for example an accelerometer to measure acceleration more accurately. The position can be retrieved from the GPS module of a smartphone. Note that the absolute position (position on earth) is a poor feature choice since it would require training on data from every exact location the system should function at.</p> <p>External data can also be used to improve predictions. One way to make position data useful is to combine it with public transport maps and timetables. By comparing the user’s position with trajectories of trams, buses and trains these modes could easily be classified. Note that this requires (real-time) web APIs for public transport routes. Some other ways to bring more data into the prediction is to compare movements to other users or earlier trips. Taking past trips into account leads to an extension of the classification problem to also infer travel patterns. I do not intend to discuss this further here, but this is an interesting problem in its own right.</p> <h3 id="machine-learning-approaches">Machine Learning Approaches</h3> <p>There are two main approaches to learning a classifier for transport modes. The first is to treat the previously mentioned data as a time series and directly feed this to the classifier. The input data then has a time-dimension of varying length and (dependent on exactly what is measured) one or more channels at each time-step. At least the speed at each time-step would be included in such an approach. In this group we find Hidden Markov Models, which is one of the most popular approaches to this problem. Another, maybe more modern, approach is Artificial Neural Networks. The typical approach to time-series classification with ANNs is using a recurrent network and LSTM units. The neural network approach is much more rare, but could be an interesting future project.</p> <p>The second main approach to the problem is to first extract features from the raw data. This opens up to a larger family of possible machine learning models since the data no longer has the shape of a time-series. Features extracted here are generally different statistical properties of the speed or acceleration throughout one trip. Common models used are decision trees, K nearest neighbor and support vector machines.</p> <h3 id="my-solution">My Solution</h3> <p>For my course project I took the main responsibility of training our classifiers. We were handed a small dataset which we had helped out collecting and labeling earlier. Because of limited time I chose to go with the second of the approaches discussed earlier and extracted a set of features from each trip to be classified.</p> <p>The features I used was inspired mainly by <a href="https://www.mdpi.com/2078-2489/6/2/212">Zong et al.</a>, with the difference that I chose not to use acceleration in the final classifier. I did some experimenting with features based on acceleration, but it didn’t lead to any substantial increase in accuracy on validation data. The features used were:</p> <ul> <li>Trip length</li> <li>Average speed</li> <li>Maximum speed</li> <li>Standard deviation of speed</li> <li>25, 50 and 75 percentiles of speed</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/scatter.svg" sizes="95vw"/> <img src="/assets/img/scatter.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Scatter plots of some of the features </div> <p>My workflow was based in two Python scripts. One for extracting features and one to validate models. I tried a multitude of models and hyperparameters, including:</p> <ul> <li>Random forests</li> <li>SVMs with rbf-kernels</li> <li>k-nn classifiers</li> </ul> <p>After removing some outliers I used k-fold cross validation to estimate generalization error of the different classifiers. After a bit of testing different models and tweaking hyperparameters I ended up with a random forest of 40 decision trees. This classifier had a balanced accuracy of ~80% on a 50-50 training-test split of the original dataset. Even though this is far from perfect I decided it was a good result given the data I had to work with. The classifier also showed good results when tested in practice on real trips.</p> <p>The fact that the dataset I worked with was fairly small was a challenge. I considered trying out the neural network approach, but it is likely that the size of the dataset would had made it very hard to train a network sufficiently. A perhaps even bigger challenge was how unbalanced the dataset was. Some classes, such as car and train had very few data points. My approach to this was to apply the <a href="https://arxiv.org/abs/1106.1813">SMOTE algorithm</a> for creating new, artificial samples from these classes. This gave a small increase in the balanced accuracy.</p> <p>Another challenge of this project was that the classifier had to be part of the final app. Since android apps are built in Java (or alternatively Kotlin) the final model had to be in Java code as well. As Java isn’t the greatest language for machine learning my approach to this was to use Python and scikit-learn for training the model, but then porting the final classifier to Java by using <a href="https://github.com/nok/sklearn-porter">sklearn-porter</a>. This worked great and I would definitely recommend sklearn-porter to anyone who prefers working with Python, but needs to port their models to some other language. The only downside of this approach was that I had to recreate the code for feature extraction both in Python and Java. For me this highlighted how we can not train machine learning models in a vacuum and as soon as we reach a good accuracy be done. Models exist as part of bigger systems and making a model work together with other parts of the system is just as important as being statistically accurate.</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/ecofootprint.png" width="300px" style="margin: auto"/> <div class="caption"> Screenshot of the final app </div> </div> <p>There are a couple related problems that we avoided tackling thoroughly in this project. These include:</p> <ul> <li>Determining when trips start and stop</li> <li>Detecting when a building is entered.</li> <li>Real time updates of transport mode belief during trip</li> <li>Handling the many brief stops of public transport as one continuous trip.</li> </ul> <h3 id="why-is-this-even-useful">Why is This Even Useful?</h3> <p>So what is even the use case for automatic transport mode classification? One possible use is exactly what my course project targeted, to give people a better understanding of energy and emissions form their transportation habits. Few people would probably be interested in such a service if it required constant manual labeling of all their trips. These sorts of decision helping systems have shown to impact users behavior and thereby save energy. The effect is even stronger if social norms are introduced, where users are compared to others.</p> <p>More in general it is not hard to see that a dataset describing detailed transportation habits could be very valuable both for individuals and organizations that they choose to share it with. There are many great opportunities for research, urban planning, public transport etc.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="transport"/><category term="app"/><category term="ml"/><summary type="html"><![CDATA[This fall I encountered the quite interesting problem of inferring what mode of transportation a person is using based on sensor data. The context for this was my smart energy course. The course is broadly about using ICT to enable sustainable energy usage and one of our assigned projects was to develop an android app to let people track their daily energy usage from transportation. For this me and my group trained a classifier based on speed measurements from the smartphone GPS.]]></summary></entry><entry><title type="html">Switzerland</title><link href="https://joeloskarsson.github.io/blog/2018/switzerland-update/" rel="alternate" type="text/html" title="Switzerland"/><published>2018-09-07T19:33:00+00:00</published><updated>2018-09-07T19:33:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2018/switzerland-update</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2018/switzerland-update/"><![CDATA[<p>This Tuesday I finally arrived to Zürich, Switzerland. I’ll be spending the next year here, studying the first year of my master at ETH Zürich.</p> <p>I’ve spent the first couple of days getting some paperwork in order and exploring the city and campus. Zürich is a very beautiful and interesting city. Next week I’ll be taking part in a project called <a href="https://www.ethz.ch/en/the-eth-zurich/sustainability/lehre/ETHweek.html">ETH Week</a>. It’s cross-disciplinary one week project and this years theme is “Energy Matters”.</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/goodbye_liu.jpg" width="300px" style="margin: auto; display: inline"/> <img src="/assets/img/hello_eth.jpg" width="300px" style="margin: auto; display: inline"/> <br/> <div class="caption"> Hej då Linköping, Guten Tag Zürich </div> </div> <p>Courses will then begin on the 17th September. I’ll mostly be doing courses related to AI in different ways. I feel very excited to be here and can’t wait for things to really get started.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="life"/><summary type="html"><![CDATA[This Tuesday I finally arrived to Zürich, Switzerland. I’ll be spending the next year here, studying the first year of my master at ETH Zürich.]]></summary></entry><entry><title type="html">Automatic online documentation for GitHub projects using Doxygen, Travis CI and GitHub pages</title><link href="https://joeloskarsson.github.io/blog/2018/automatic-docs/" rel="alternate" type="text/html" title="Automatic online documentation for GitHub projects using Doxygen, Travis CI and GitHub pages"/><published>2018-08-24T14:47:00+00:00</published><updated>2018-08-24T14:47:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2018/automatic-docs</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2018/automatic-docs/"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <p>For a couple of weeks I’ve been putting my evenings into developing a neural network framework in C++. As the project has grown I’ve started thinking about how useful it would be to have an accessible documentation at hand. This immediately got me thinking about doxygen, a really nice tool for generating documentation in multiple different formats directly from comments in source code. By combining this with the Continuous Integration tool Travis CI and web hosting from GitHub pages I set up a system that keeps an online documentation up to date with the master branch of the project. This setup can quite easily be expanded and generalised to any kind of project hosted on GitHub.</p> <p>The main idea behind the system is:</p> <ul> <li>We have a set of source files with descriptive comments</li> <li>html documentation for these is generated through Doxygen</li> <li>Travis CI runs Doxygen on every push</li> <li>Documentation is pushed to separate repository/branch</li> <li>Repository/branch is served using GitHub pages</li> </ul> <p>An example of this setup can be found in my <a href="https://github.com/joeloskarsson/NNetCpp">NNetCpp</a> project, with the documentation pushed to the <a href="https://github.com/joeloskarsson/NNetDocs">NNetDocs repository</a>. This can then be reached through a <a href="https://joeloskarsson.github.io/NNetDocs/">GitHub pages website</a>.</p> <h3 id="source-files">Source files</h3> <p>Although this setup will help with an easy deployment of the documentation, the quality of the documentation is fully dependent on writing informative comments. Doxygen allows for many different ways to comment your code. Everything from just a short sentence describing a function to entire constructs using keywords like \param and \return are supported. Take a look at the <a href="https://doxygen.nl/manual/docblocks.html">Doxygen documentation</a> for how to comment your code to allow for generating documentation.</p> <h3 id="doxygen">Doxygen</h3> <p>Doxygen is easy to configure and run. Downloads can be found on the <a href="https://www.doxygen.nl/download.html">website</a>. On Debian-based linux distributions it can simply be installed with</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>apt-get <span class="nb">install </span>doxygen</code></pre></figure> <p>To use Doxygen with a project the only thing needed is a configuration file. In the project folder, run</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">doxygen <span class="nt">-g</span> &lt;config-file-name&gt;</code></pre></figure> <p>to generate a default one. This is a pretty big file and might seem frightening at first, but luckily we don’t have to change that much in it. It is also very extensively documented, making it easy to understand the different options.</p> <p>This is a good time to consider the directory structure. This is the structure I’ve been using for my project.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">ProjectFolder/
    +- doxygen.conf             Doxygen config file
    +- Makefile                 Makefile for project
    +- src/                     Source file directory
    +- documentation/           Documentation directory
        +- docs/                Directory for html documentation
        +- (Documentation.pdf)  (pdf documentation, if generated)</code></pre></figure> <p>With this setup the documentation is kept in a completely separate directory. This is beneficial to keep our project top folder clean. It is also the <code class="language-plaintext highlighter-rouge">documentation</code> directory that we eventually want to serve using GitHub pages. Other project structures can of course also be used by adjusting some of the paths in upcoming steps.</p> <p>We need to adjust some settings in our Doxygen config. These options are spread out throughout the file, so the easiest way to find them is to search for them.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">PROJECT_NAME           = "My Cool Project"</code></pre></figure> <p>The name of the project. Used for headers in the documentation.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">PROJECT_BRIEF          = "A short description of my project"</code></pre></figure> <p>Brief description of project. Used as subheader in documentation.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">OUTPUT_DIRECTORY       = "documentation"</code></pre></figure> <p>Directory to output all documentation into. Adjust this if you want a different directory structure.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">INPUT                  = src</code></pre></figure> <p>Directories containing source files and direct paths to source files. All files to be included in the documentation needs to included here.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">RECURSIVE              = YES</code></pre></figure> <p>If you have subdirectories in your source folders that you want to be included in the documentation set this to YES.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">GENERATE_HTML          = YES</code></pre></figure> <p>This tells Doxygen to create a html documentation. (YES is already default)</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">HTML_OUTPUT            = docs</code></pre></figure> <p>Output folder of HTML documentation. The name <code class="language-plaintext highlighter-rouge">docs</code> later allows us to serve this folder specifically through GitHub pages.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">GENERATE_LATEX         = NO</code></pre></figure> <p>For HTML documentation we don’t need this. See the chapter further down on how to extend this setup to also create a PDF documentation document.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">HAVE_DOT               = NO</code></pre></figure> <p>Doxygen uses the dot tool to generate class diagrams. If you have it installed you can set this to YES. I did not feel like installing it, so I set it to NO to generate more simple diagrams.</p> <p>There are plenty more options in the Doxygen config file that can be used to customize the documentation. Most can be understood by just reading the comments above them. For more in depth information see the <a href="https://www.stack.nl/~dimitri/doxygen/manual/index.html">Doxygen documentation</a>.</p> <p>To finally generate the documentation we simply run</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">doxygen doxygen.conf</code></pre></figure> <p><code class="language-plaintext highlighter-rouge">doxygen.conf</code>is the name of the Doxygen config file. This should create html documentation in the <code class="language-plaintext highlighter-rouge">documentation/html</code> directory. To look at it locally simply open <code class="language-plaintext highlighter-rouge">documentation/html/index.html</code> in a browser.</p> <p>Since I have been using a Makefile for the rest of my project I decided to add the generation of documentation as a command in it:</p> <figure class="highlight"><pre><code class="language-makefile" data-lang="makefile"><span class="nl">.PHONY</span><span class="o">:</span> <span class="nf">docs</span>

<span class="nl">docs</span><span class="o">:</span>
	doxygen doxygen.conf</code></pre></figure> <p>With this we can simply run <code class="language-plaintext highlighter-rouge">make docs</code> to generate the documentation.</p> <h3 id="travis-ci">Travis CI</h3> <p>Travis CI is a Continuous Integration tool that can be used freely with open source projects on GitHub. It is mainly used for testing and deploying builds, but can be set up to do almost anything after a push. In this setup we will use it to generate the documentation and then push it to a separate repository/branch.</p> <p>I prefer having a completely separate GitHub repository for the documentation. This allows for better separation between code and docs. Another benefit is that since the name in the GitHub pages URL is the same as the name of the repository we can then choose it freely on the documentation repository. You might prefer to deploy the documentation to the same project repository on the gh-pages branch. This can easily be achieved by changing some values in the Travis configuration. Note however that you then have to make sure that the html documentation is in the root folder for GitHub to serve it correctly.</p> <p>To use Travis with your project you need to authorize it for use with your project repository. Follow the <a href="https://docs.travis-ci.com/user/getting-started/">Getting Started</a> page of the Travis CI documentation to get set up.</p> <p>Travis is configured through a <code class="language-plaintext highlighter-rouge">.travis.yml</code> file in the repository. Travis runs on a virtual machine that pulls the repository and then runs multiple commands. In this file we need to prepare this virtual machine, generate the documentation and deploy it to our decided destination.</p> <p>The entire <code class="language-plaintext highlighter-rouge">.travis.yml</code> file looks like this</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">language</span><span class="pi">:</span> <span class="s">cpp</span>

<span class="na">branches</span><span class="pi">:</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">master</span>

<span class="na">sudo</span><span class="pi">:</span> <span class="s">enabled</span>

<span class="na">before_install</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">sudo apt-get update</span>
      <span class="pi">-</span> <span class="s">sudo apt-get install -y doxygen</span>

<span class="na">script</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">make docs</span>

<span class="na">deploy</span><span class="pi">:</span>
    <span class="na">provider</span><span class="pi">:</span> <span class="s">pages</span>
    <span class="na">skip-cleanup</span><span class="pi">:</span> <span class="kc">true</span>
    <span class="na">github-token</span><span class="pi">:</span> <span class="s">$GITHUB_TOKEN</span>
    <span class="na">keep-history</span><span class="pi">:</span> <span class="kc">true</span>
    <span class="na">on</span><span class="pi">:</span>
        <span class="na">branch</span><span class="pi">:</span> <span class="s">master</span>
    <span class="na">local-dir</span><span class="pi">:</span> <span class="s">documentation</span>
    <span class="na">repo</span><span class="pi">:</span> <span class="s">&lt;github-username&gt;/&lt;documentation-repository&gt;</span>
    <span class="na">target-branch</span><span class="pi">:</span> <span class="s">master</span></code></pre></figure> <p>Here’s what the different commands mean:</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">language</span><span class="pi">:</span> <span class="s">cpp</span></code></pre></figure> <p>The programming language used in the project. Here it is C++.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">branches</span><span class="pi">:</span>
    <span class="na">only</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">master</span></code></pre></figure> <p>Only deploy documentation on pushes to master branch. Change this to fit with your branch workflow.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">sudo</span><span class="pi">:</span> <span class="s">enabled</span></code></pre></figure> <p>Allow us to use sudo for installing packages.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">before_install</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">sudo apt-get update</span>
      <span class="pi">-</span> <span class="s">sudo apt-get install -y doxygen</span></code></pre></figure> <p>This is where we customize the virtual machine to support our scripts. Update package infrastructure and install Doxygen. <code class="language-plaintext highlighter-rouge">-y</code> is for automatically answering yes to the install prompt.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">script</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">make docs</span></code></pre></figure> <p>Our script. Since we put the command in the Makefile it is enough to run <code class="language-plaintext highlighter-rouge">make docs</code>. If not using a Makefile you could easily put it in a bash-script or simply type out <code class="language-plaintext highlighter-rouge">doxygen doxygen.conf</code> in the <code class="language-plaintext highlighter-rouge">.travis.yml</code>.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">deploy</span><span class="pi">:</span>
    <span class="na">provider</span><span class="pi">:</span> <span class="s">pages</span></code></pre></figure> <p>This tells Travis that we want to deploy using GitHub pages. Travis has built in support for this, but it requires some setup from our side. For more information about deploying to GitHub pages see the <a href="https://docs.travis-ci.com/user/deployment/pages/">Travis docs</a>.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">skip-cleanup</span><span class="pi">:</span> <span class="kc">true</span></code></pre></figure> <p>Do not remove the files that we want to upload.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">github-token</span><span class="pi">:</span> <span class="s">$GITHUB_TOKEN</span></code></pre></figure> <p>In order to allow Travis to commit to our repositories we need to give it a personal access token. The way to do this in a secure way is to add the token as a hidden environment variable in the Travis repository settings.</p> <p>First go to your GitHub settings page. Under developer settings generate a new personal access token. If the repository you want to deploy to is public you only need the <em>public_repo</em> scope. If it is private you need <em>repo</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/access_token-480.webp 480w,/assets/img/access_token-800.webp 800w,/assets/img/access_token-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/access_token.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Generating a new personal access token </div> <p>Copy this token and go to the Travis CI repository settings for you project repository. Create a new environment variable named <code class="language-plaintext highlighter-rouge">GITHUB_TOKEN</code> and set the value to your personal access token. Make sure the <em>Display value in build log</em> slider is set to off so your token stays hidden.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/travis_var-480.webp 480w,/assets/img/travis_var-800.webp 800w,/assets/img/travis_var-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/travis_var.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> GITHUB_TOKEN environment variable in Travis repository settings </div> <p>Now Travis should be allowed to commit to our documentation repository.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">keep-history</span><span class="pi">:</span> <span class="kc">true</span></code></pre></figure> <p>Tells Travis to not force push, but keep the history of the repository.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">on</span><span class="pi">:</span>
        <span class="na">branch</span><span class="pi">:</span> <span class="s">master</span></code></pre></figure> <p>Deploy when the push is to the master branch.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">local-dir</span><span class="pi">:</span> <span class="s">documentation</span></code></pre></figure> <p>Select what directory to push to the documentation repository. With the directory structure set up as described earlier we simply want to tell Travis to push the entire <code class="language-plaintext highlighter-rouge">documentation</code> directory.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">repo</span><span class="pi">:</span> <span class="s">&lt;github-username&gt;/&lt;documentation-repository&gt;</span></code></pre></figure> <p>The repository to push the documentation to. Written on the form <em>username/reponame</em>.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="na">target-branch</span><span class="pi">:</span> <span class="s">master</span></code></pre></figure> <p>The branch to push to on the documentation repository.</p> <p>With this set up we can then push to our master branch and Travis will start a new job. Travis will generate the documentation and then push it to the documentation repository. If the Travis build would fail make sure to check the log file. From the log it should be clear what went wrong.</p> <h3 id="github-pages">GitHub Pages</h3> <p>Now we have our documentation in a repository ready to go. We only have to tell GitHub to start serving it using GitHub pages.</p> <p>In the documentation repository, go to the settings tab. Scroll down to GitHub pages and select source <em>master branch /docs folder</em>. This is the reason why we chose the name <code class="language-plaintext highlighter-rouge">docs</code> for the html documentation directory in the Doxygen config.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gh_pages-480.webp 480w,/assets/img/gh_pages-800.webp 800w,/assets/img/gh_pages-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gh_pages.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> GitHub pages set up to serve to /docs directory from the master branch </div> <p>Wait for a second and your documentation should be available at <code class="language-plaintext highlighter-rouge">&lt;github-username&gt;.github.io/&lt;repository-name&gt;</code>. Now we have a finished system that will update the documentation after each push to the master branch.</p> <p>Everything should work as expected at this point, but once everything is set up we can expand on the system in different ways.</p> <h3 id="pdf-from-latex">PDF from \(\LaTeX\)</h3> <p>One thing that could be nice is to also include the documentation as a pdf file in the documentation repository. This can be achieved by configuring Doxygen to output in LaTeX format and then using a LaTeX compiler to create a pdf file.</p> <p>The only change needed in the Doxygen config is:</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">GENERATE_LATEX         = YES</code></pre></figure> <p>This tells Doxygen to generate a LaTeX documentation in the <code class="language-plaintext highlighter-rouge">latex</code> directory.</p> <p>For compiling this documentation we need a LaTeX compiler and some extra packages. A common one is TeX Live, easiest installed on debian systems with</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>apt-get <span class="nb">install </span>texlive texlive-latex-extra</code></pre></figure> <p>We can now look at the pdf documentation by running</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">doxygen doxygen.conf
<span class="nb">cd </span>documentation/latex
make</code></pre></figure> <p>This should create the file <code class="language-plaintext highlighter-rouge">refman.pdf</code> in the <code class="language-plaintext highlighter-rouge">latex</code> folder.</p> <p>To automate this process and make sure that the pdf is pushed to our documentation repository we need to put this in a script. Once we are done with the LaTeX files they are unnecessary and should be deleted. I edited my Makefile to create the pdf, move it out of the latex folder and delete it.</p> <figure class="highlight"><pre><code class="language-makefile" data-lang="makefile"><span class="nl">.PHONY</span><span class="o">:</span> <span class="nf">docs</span>

<span class="nl">docs</span><span class="o">:</span>
	doxygen doxygen.conf
	<span class="nb">cd </span>documentation/latex <span class="o">&amp;&amp;</span> make
	<span class="nb">mv </span>documentation/latex/refman.pdf documentation/Documentation.pdf
	<span class="nb">rm</span> <span class="nt">-r</span> documentation/latex</code></pre></figure> <p>With this in the same Makefile command as before we don’t have to adjust the Travis config much. The only necessary change to <code class="language-plaintext highlighter-rouge">.travis.yml</code> is the addition of the LaTeX packages.</p> <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">before_install</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">sudo apt-get update</span>
      <span class="pi">-</span> <span class="s">sudo apt-get install -y doxygen texlive texlive-latex-extra</span></code></pre></figure> <p>Now Travis will also create a pdf documentation for you. The file <code class="language-plaintext highlighter-rouge">Documentation.pdf</code> can be found in the root of the documentation repository.</p> <p>There are many other ways to adapt and expand on this setup for the needs of different projects. If you have any questions, ideas for improvements or cool use-cases feel free to reach out.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="ci"/><category term="documentation"/><category term="github"/><summary type="html"><![CDATA[For a couple of weeks I’ve been putting my evenings into developing a neural network framework in C++. As the project has grown I’ve started thinking about how useful it would be to have an accessible documentation at hand. This immediately got me thinking about doxygen, a really nice tool for generating documentation in multiple different formats directly from comments in source code. By combining this with the Continuous Integration tool Travis CI and web hosting from GitHub pages I set up a system that keeps an online documentation up to date with the master branch of the project. This setup can quite easily be expanded and generalised to any kind of project hosted on GitHub.]]></summary></entry><entry><title type="html">An analysis of dependencies to software packages in open source JavaScript projects</title><link href="https://joeloskarsson.github.io/blog/2018/npm-dependencies/" rel="alternate" type="text/html" title="An analysis of dependencies to software packages in open source JavaScript projects"/><published>2018-07-30T19:57:00+00:00</published><updated>2018-07-30T19:57:00+00:00</updated><id>https://joeloskarsson.github.io/blog/2018/npm-dependencies</id><content type="html" xml:base="https://joeloskarsson.github.io/blog/2018/npm-dependencies/"><![CDATA[<p>I recently completed my bachelor’s project and thesis. As part of the thesis work I performed an analysis on dependencies in the npm ecosystem. Since the thesis is in Swedish and therefore quite inaccessible I thought I would write up a summary in English. This post will not translate the chapter in its entirety, but rather summarize the most interesting quantitative results and some insights.</p> <h1 id="npm">npm</h1> <p>I doubt anyone working on web technology has managed to stay away from the Node Package Manager (npm), a repository filled with JavaScript packages supplying a multitude of functionalities. Npm has grown at a fascinating pace and continues to do so. The rise of npm and similar repositories for other languages has resulted in new ways of building software. Developers are able to do less of reinventing the wheel and more of putting together the wheels they found through npm into really nice cars (web apps).</p> <p>Having this rich set of packages available has opened up many opportunities and made it easier to create great web apps. This way of working, gluing together code written by others into a finished product, does however raise some concerns. If we are not in charge of the codebase, how can we assure quality? Especially matters of security and maintainability are of interest.</p> <p>I do touch on the security concern in the thesis, but I choose to not focus on that here. Instead I recommend taking a look the <a href="https://www.researchgate.net/publication/279196437_In_Dependencies_We_Trust_How_vulnerable_are_dependencies_in_software_modules">work of Joseph Hejderup</a>.</p> <p>My main focus has not been to respond to these concerns directly, but rather to establish a picture of how widespread the dependency use is within JavaScript software. To see if these concerns are valid we need to further understand the typical use of dependencies to other npm packages.</p> <h1 id="dependencies">Dependencies</h1> <p>JavaScript projects using npm list their dependencies in a file called <em>package.json</em>. These are the direct dependencies of a project. Packages in the npm repository can also have own dependencies, subdependencies to other packages. The result of this is a directed graph structure, as can be seen below. All nodes reachable from the original project can be considered indirect dependencies.</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/dependency_graph.png" width="400px" style="margin: auto"/> <br/> <span>Example of npm dependencies modeled as directed graph</span> </div> <p>Npm classifies dependencies in two different groups, <em>dependencies</em> and <em>dev-dependencies</em>. Dependencies are packaged in the final product and the code can be seen as part of the project. Dev-dependencies are packages used in the development of a project. Note for example that security concerns in dev-dependencies should generally not be considered as severe as security concerns in dependencies (although still of importance).</p> <h1 id="method">Method</h1> <p>The method used was mining of software repositories. Open source projects on github were searched for package.json files containing lists of dependencies. This resulted in data on direct dependencies. For subdependencies to packages the npm command line tool was used to retrieve information. For storing all the data an SQLite database was used, as can be seen below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/npm_db-480.webp 480w,/assets/img/npm_db-800.webp 800w,/assets/img/npm_db-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/npm_db.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Database used in analysis </div> <p>This database can be seen as storing the directed graph model representing dependencies between packages. By storing package dependency data investigation of the same package multiple times could be avoided. For executing the actual analysis a Python3 script was used.</p> <h1 id="results">Results</h1> <p>The script was ran for three days, resulting in data on over 6000 projects (all JavaScript projects on github with over 500 starts rating at the time). The three statistics that were concluded from this data are direct dependencies, indirect dependencies and depth of dependency chains.</p> <p>Let’s first take a look at direct dependencies. (Axis label translation: x-axis - Direct dependencies, y-axis - Projects)</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/direct_dep.png" width="500px" style="margin: auto"/> <div class="caption"> Direct dependencies </div> </div> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/direct_dep_dev.png" width="500px" style="margin: auto"/> <div class="caption"> Direct dev-dependencies </div> </div> <p>It is clear that a majority of the analysed JavaScript projects have only a few dependencies. 27% had no dependencies at all. Very few have more than 20. The mean amount of direct dependencies was 6.76.</p> <p>Looking at direct dev-dependencies the amounts are clearly higher. Still many projects have only a few, but the decrease in the histogram is somewhat slower. Do keep in mind different scalings of the y-axis when comparing graphs.</p> <p>Following the dependencies of packages we get data on the amounts of indirect dependencies. (Axis label translation: x-axis - Indirect dependencies, y-axis - Projects)</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/indirect_dep.png" width="500px" style="margin: auto"/> <br/> <div class="caption"> Indirect dependencies </div> </div> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/indirect_dep_dev.png" width="500px" style="margin: auto"/> <br/> <div class="caption"> Indirect dev-dependencies </div> </div> <p>For indirect dependencies we can see a similar pattern as direct with the majority of projects depending on only a few packages. Almost none seem to depend on more than 500 packages indirectly.</p> <p>For dev-dependencies we see a much more flat distribution. Still many projects have very few indirect dev-dependencies. There is however hundreds of packages in the analysed set depending on more than 1000 packages indirectly.</p> <p>Another thing of interest is the depth of these chains of dependencies, from the project through packages to their sub-dependencies. This might give an idea of how easy it is for the developer to understand what dependencies their project actually rely on. (Axis label translation: x-axis - Dependency depth, y-axis - Projects)</p> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/dep_depth.png" width="500px" style="margin: auto"/> <br/> <div class="caption"> Dependency depth </div> </div> <div style="text-align: center; margin: 20px 0px;"> <img src="/assets/img/dep_depth_dev.png" width="500px" style="margin: auto"/> <br/> <div class="caption"> Dev-dependency depth </div> </div> <p>Here we can see the projects missing dependencies with 0 depth. Despite quite low numbers of indirect dependencies we still see that a dependency depth of more than 10 is fairly common.</p> <p>If we look at dev-dependencies it is clear that there are a some big development packages relying on many others. With almost as many packages at 16 and 17 depth as 0 it is clear that dev-dependencies can lead to massive chains of packages. Notice also how very few projects have chains of dev-dependencies only a few steps long. A vast majority of those above 0 have a length of over 7.</p> <h1 id="some-insights">Some insights</h1> <p>There is quite some difference between the amount and complexity of dependencies and dev-dependencies in the analysed projects. It is clear that there are big frameworks often being used on the developer side, indirectly adding on a number of dev-dependencies. This might at first seem quite worrying for the quality of JavaScript projects, but it is important to consider that these dependencies have no role in the final software. One might even say that this simply indicates that JavaScript developers tend to use a lot of powerful tools (we’ll assume that is a good thing).</p> <p>One of the biggest improvements (or expansions) that could be done to the analysis I think would be to categorize the examined projects. I suspect many of the projects are themselves npm packages. When considering security concerns we might mainly be interested in what would be considered end-user web applications. A problem might be that few codebases of that type are available openly for analysis. Removing all the npm packages from the data might also change the high amount of projects with few or no dependencies.</p> <p>So is JavaScript software heavy and fragile from all the dependencies weighing it down and exposing new weaknesses? I would be very careful to generalise this analysis to conclusions about JavaScript software in general. What we do see in this set of projects though is quite a range of different amounts of dependencies. Still, with a direct dependency average of less than 7 any issue does not seem present here. Staying up to date with that few packages should be no issue for most developer teams. Even if we consider indirect dependencies up to 100 assuring quality even through dependencies should be possible with the right tools and processes.</p> <p>For more details see the original <a href="http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-149042">thesis</a> (unfortunately in Swedish). Chapter F about my analysis can be found at page 80. If you are interested in further information in English feel free to get in touch.</p>]]></content><author><name>Joel Oskarsson</name></author><category term="thesis"/><category term="javascript"/><category term="npm"/><summary type="html"><![CDATA[I recently completed my bachelor’s project and thesis. As part of the thesis work I performed an analysis on dependencies in the npm ecosystem. Since the thesis is in Swedish and therefore quite inaccessible I thought I would write up a summary in English. This post will not translate the chapter in its entirety, but rather summarize the most interesting quantitative results and some insights.]]></summary></entry></feed>